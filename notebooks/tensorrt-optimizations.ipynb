{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cZul_DVAHE0f"
   },
   "source": [
    "# Optimization of TensorFlow models with TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yLLqSh8yHE0g"
   },
   "source": [
    "In this notebook you will learn how to use the TensorFlow integration for TensorRT (also known as TF-TRT) to increase inference performance.\n",
    "\n",
    "For more information and detail about [TF-TRT](https://blog.tensorflow.org/2019/06/high-performance-inference-with-TensorRT.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MpTTFSiAHE0g"
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TX5LaD6HE0h"
   },
   "source": [
    "By the time you complete this workshop you will be able to:\n",
    "\n",
    "- Optimize several deep learning models with TF-TRT\n",
    "- Describe how TF-TRT optimizes models\n",
    "- Use TF-TRT to optimize models at FP32 precision\n",
    "- Use TF-TRT to optimize models at FP16 precision\n",
    "- Perform calibration for INT8 precision optimization\n",
    "- Perform experiments to understand the impact of conversion parameters on optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nX69v5IhHUF5"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "Jq1QQSUbHa0g",
    "outputId": "6c4af26f-507b-488c-9eee-2d40ba13dfaa"
   },
   "outputs": [],
   "source": [
    "Image(\"images/pipeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0hGwUi_QHE0j"
   },
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KhTIYq8ZHE0k"
   },
   "source": [
    "This workshop consists of several JupyterLab Notebooks.\n",
    "\n",
    "- **A quick overview of how to work with thi Colaboratory environment**\n",
    "- **Review inference with TF 2 and get familiar with helper functions used in this workshop**\n",
    "- **Learn how TF-TRT optimizes models for faster inference**\n",
    "- **Learn the syntax for performing optimization with TF-TRT**\n",
    "- **Perform FP16 precision optimization** \n",
    "- **Learn how TF-TRT optimizes with INT8 precision** \n",
    "- **Perform data calibration and optimize with INT8 precision** \n",
    "- **Experiment with the impact of the minimum segment size conversion parameter, and optimize additional models** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8JZW1AvPHE0n"
   },
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yHIw8cP2HE0o",
    "outputId": "6f9d63d6-c73b-42fb-d7b3-c7c2b19c3d91"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rSnej3X5HE0t"
   },
   "source": [
    "As you can see almost no GPU memory is being used right now, and, there are no active processes utilizing the GPUs. Throughout the lab you can use this command to keep an eye on memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "czCHQSSfHE0u"
   },
   "source": [
    "# Naive Inference with TensorFlow 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "25vfNfKrHE0v"
   },
   "source": [
    "In this notebook we will run inference with TensorFlow 2, without the help of TF-TRT. In doing so we will establish baselines for image throughput and prediction accuracy which we can use as we optimze with TF-TRT.\n",
    "\n",
    "Additionally, we will spend some time getting familiar with several helper functions we will use throughout this workshop that will allow us to perform common tasks easily so we can focus on the impact of using TF-TRT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A59nGxzRHE0v"
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94Z7U_RNHE0w"
   },
   "source": [
    "By the time you complete this notebook you should be able to:\n",
    "\n",
    "- Use provided helper functions to load images, batch input, make and benchmark predictions, and display prediction information\n",
    "- Obtain a baseline for naive TensorFlow 2 inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LsFtdjqgHE0w"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bsc2rnS2HE0w"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import tag_constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o7FeF_ldHE0z"
   },
   "source": [
    "Throughout the notebook we will make extensive use of helper functions defined in `./lab_helpers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYAJjrcuHE0z"
   },
   "outputs": [],
   "source": [
    "from lab_helpers import (\n",
    "    get_images, batch_input, predict_and_benchmark_throughput_from_saved, display_prediction_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CttlYILpHE02"
   },
   "source": [
    "## Load and Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTgH0EzbHE03"
   },
   "source": [
    "Thoughout much of this workshop we will be using ResNetV2. Here we import the model from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KwwactV4HE03"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet_v2 import ResNet152V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SY-hKVbEHE05"
   },
   "outputs": [],
   "source": [
    "model = ResNet152V2(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RoT7OMtMHE07"
   },
   "source": [
    "When we benchmark our optimized TF-TRT models, they will be saved TensorFlow (not Keras) models. In order to have a fair comparison, here we save our Keras model as a TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2HkwHkoHE08"
   },
   "outputs": [],
   "source": [
    "tf.saved_model.save(model, 'resnet_v2_152_saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aOZ-ekcIHE0-"
   },
   "source": [
    "## Create Batched Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rl4BYSiHE0_"
   },
   "source": [
    "Using **batch inference** to send many images to the GPU at once promotes parallel processing and improve throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VujHs8x9HE0_"
   },
   "source": [
    "### Get Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28Bj5L9ZHE1A"
   },
   "source": [
    "The `get_images` helper function will use Keras to load the number of images specified, returning for each image the image itself in PIL format, and its file path, which we will need later to load and view the images from within these notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b9Bp6NIMHE1A"
   },
   "outputs": [],
   "source": [
    "number_of_images = 32\n",
    "images = get_images(number_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnQYNPg9HE1C"
   },
   "outputs": [],
   "source": [
    "images[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5fOCuaXLHE1E"
   },
   "source": [
    "### Batch Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQIe__t9HE1E"
   },
   "source": [
    "The `batch_input` helper function takes a list of images with their paths, as returned by `get_images`, and returns a tensor with the the images preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RiBOFPEAHE1F"
   },
   "outputs": [],
   "source": [
    "batched_input = batch_input(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhHipAdNHE1G"
   },
   "outputs": [],
   "source": [
    "type(batched_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_gs1340cHE1J"
   },
   "outputs": [],
   "source": [
    "batched_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-BoTpIyHE1M"
   },
   "source": [
    "## Get Baseline for Prediction Throughput and Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sIrvK1GkHE1M"
   },
   "source": [
    "The following will serve as a baseline for prediction throughput and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwnHw-YzHE1M"
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g7eg-ds-HE1N"
   },
   "source": [
    "Here we load a previously-saved ResnetV2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a--ZEzD2HE1N"
   },
   "outputs": [],
   "source": [
    "def load_tf_saved_model(input_saved_model_dir):\n",
    "\n",
    "    print('Loading saved model {}...'.format(input_saved_model_dir))\n",
    "    saved_model_loaded = tf.saved_model.load(input_saved_model_dir, tags=[tag_constants.SERVING])\n",
    "    return saved_model_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0I9vObTHE1P"
   },
   "outputs": [],
   "source": [
    "saved_model_loaded = load_tf_saved_model('resnet_v2_152_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhft_GffHE1R"
   },
   "outputs": [],
   "source": [
    "infer = saved_model_loaded.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EGkiUmGHE1S"
   },
   "source": [
    "### Make Prediction and Get Throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uB-_TZwqHE1T"
   },
   "source": [
    "Now we perform inference with the optimized graph, and after a warmup, time and calculate throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_2rc4V2oHE1X"
   },
   "source": [
    "The helper functoin `predict_and_benchmark_throughput_from_saved` will use the passed in model to perform predictions on the passed in batched input over a number of runs. It measures and reports throughput, as well as time for ranges of runs.\n",
    "\n",
    "Because, due to GPU initialization operations, we do not want to profile against initial inference, we can set a number of warmup runs to perform prior to benchmarking.\n",
    "\n",
    "`predict_and_benchmark_throughput_from_saved` returns the predictions for all images for all runs, after the warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQyZiZTuHE1X"
   },
   "outputs": [],
   "source": [
    "all_preds = predict_and_benchmark_throughput_from_saved(batched_input, infer, N_warmup_run=50, N_run=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "75sgglQMHE1Z"
   },
   "source": [
    "**Make note of the *Throughput* value for this naive TensorFlow 2 inference.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "moKj44I9HE1Z"
   },
   "source": [
    "### Observe Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DjAIivLHE1a"
   },
   "outputs": [],
   "source": [
    "last_run_preds = all_preds[0]\n",
    "display_prediction_info(last_run_preds, images, top=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-Yy797wHE1d"
   },
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rsN3p2jwHE1d"
   },
   "source": [
    "In the next notebook you will learn how TF-TRT optimizes saved models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hi_Gep47HE1d"
   },
   "source": [
    "# TF-TRT Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBrkh_PqHE1e"
   },
   "source": [
    "This notebook gives a high-level description of how TF-TRT optimizes graphs, and also, how to code this optimization. In the next notebook you will use this knowledge to code a TF-TRT optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ui_RpJX5HE1e"
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e7r-OsNnHE1e"
   },
   "source": [
    "By the time you complete this notebook you will be able to:\n",
    "\n",
    "- Describe how TF-TRT performs graph optimization\n",
    "- Describe how to use `TrtGraphConverterV2` to code TF-TRT graph optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2REEvAqHE1f"
   },
   "source": [
    "## Network Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4IlJHRWbHE1f"
   },
   "source": [
    "TF-TRT performs several important transformations and optimizations to the neural network graph. First, layers with unused outputs are eliminated to avoid unnecessary computation. Next, where possible, convolution, bias, and ReLU layers are fused to form a single layer. *Figure 1* shows a typical convolutional network before optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 696
    },
    "colab_type": "code",
    "id": "DiVKQiCQJIKv",
    "outputId": "f08a6e1e-8eec-4252-e248-391faf02ea44"
   },
   "outputs": [],
   "source": [
    "Image(\"images/network_optimization.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "colab_type": "code",
    "id": "qu659QKMJJRr",
    "outputId": "7256e64c-af71-4d3d-ac9e-152b299d2127"
   },
   "outputs": [],
   "source": [
    "Image(\"images/network_vertical_fusion.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 715
    },
    "colab_type": "code",
    "id": "HgtUIRj0JJzi",
    "outputId": "b54c6995-e341-4bd2-bf8e-df664e75471d"
   },
   "outputs": [],
   "source": [
    "Image(\"images/network_horizontal_fusion.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8t6ojMiHE1g"
   },
   "source": [
    "When optimizing a TensorFlow model, TF-TRT can optimize either a subgraph or the entire graph definition. This capability allows the optimization procedure to be applied to the graph where possible and skip the non-supported graph segments. As a result, if the existing model contains a non-supported layer or operation, TensorFlow can still optimize the graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0265wPP5HE1h"
   },
   "source": [
    "Please see the [TF-TRT User Guide](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops) for a full list of supported operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XMOZI2BmHE1h"
   },
   "source": [
    "## TF-TRT Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKGSvTgDHE1h"
   },
   "source": [
    "Below, you can see a typical workflow of TF-TRT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "nC7TN5YVJXcY",
    "outputId": "5e5f98d1-9135-4742-a020-2f11176abda1"
   },
   "outputs": [],
   "source": [
    "Image(\"images/inference_process_fp.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O2JHCgX-HE1i"
   },
   "source": [
    "We now turn to the syntax for this one additional *Convert to TF-TRT* step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYK1shh3HE1i"
   },
   "source": [
    "## Graph Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R6dAIP1HHE1j"
   },
   "source": [
    "To perform graph conversion, we use `TrtGraphConverterV2`, passing it the directory of a saved model, and any updates we wish to make to its conversion parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ki0ZtCHTHE1j"
   },
   "source": [
    "```python\n",
    "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
    "\n",
    "trt.TrtGraphConverterV2(\n",
    "    input_saved_model_dir=None,\n",
    "    conversion_params=TrtConversionParams(precision_mode='FP32',\n",
    "                                          max_batch_size=1\n",
    "                                          minimum_segment_size=3,\n",
    "                                          max_workspace_size_bytes=1073741824,\n",
    "                                          use_calibration=True,\n",
    "                                          maximum_cached_engines=1,\n",
    "                                          is_dynamic_op=True,\n",
    "                                          rewriter_config_template=None,\n",
    "                                         )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJxqN9IqHE1j"
   },
   "source": [
    "### Conversion Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "538kEvvYHE1k"
   },
   "source": [
    "Another transformation is horizontal layer fusion, or layer aggregation, along with the required division of aggregated layers to their respective outputs, as Figure 3 shows.\n",
    "\n",
    "Horizontal layer fusion improves performance by combining layers that take the same source tensor and apply the same operations with similar parameters, resulting in a single larger layer for higher computational efficiency. The example in Figure 3 shows the combination of 3 1×1 CBR layers from Figure 2 that take the same input into a single larger 1×1 CBR layer. Note that the output of this layer must be disaggregated to feed into the different subsequent layers from the original input graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-FvjlnFHE1k"
   },
   "source": [
    "Here is additional information about the most frequently adjusted conversion parameters, all of which you will have an opportunity to code with in later exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mSv9dX9fHE1k"
   },
   "source": [
    "* __precision_mode__: This parameter sets the precision mode; which can be one of FP32, FP16, or INT8. Precision lower than FP32, meaning FP16 and INT8, would improve the performance of inference. The FP16 mode uses Tensor Cores or half precision hardware instructions, if possible. The INT8 precision mode uses integer hardware instructions.\n",
    "\n",
    "* __max_batch_size__: This parameter is the maximum batch size for which TF-TRT will optimize. At runtime, a smaller batch size may be chosen, but, not a larger one.\n",
    "\n",
    "* __minimum_segment_size__: This parameter determines the minimum number of TensorFlow nodes in a TF-TRT engine, which means the TensorFlow subgraphs that have fewer nodes than this number will not be converted to TensorRT. Therefore, in general, smaller numbers such as 5 are preferred. This can also be used to change the minimum number of nodes in the optimized INT8 engines to change the final optimized graph to fine tune result accuracy.\n",
    "\n",
    "* __max_workspace_size_bytes__: TF-TRT operators often require temporary workspace. This parameter limits the maximum size that any layer in the network can use. If insufficient scratch is provided, it is possible that TF-TRT may not be able to find an implementation for a given layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1sM94aTuHE1l"
   },
   "source": [
    "For more information, please refer to the [TF-TRT User Guide](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "14Z53YflHE1l"
   },
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I5SHjjjZHE1l"
   },
   "source": [
    "In the next notebook we will demonstrate a TF-TRT conversion using Float32 precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMIXwnfEHE1m"
   },
   "source": [
    "# Convert to TF-TRT Float32 and Float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S1GbdaAoHE1m"
   },
   "source": [
    "In this notebook we will demonstrate how to convert a TensorFlow saved model into a TF-TRT optimized graph using Float32 and Float16 precision. We will use the optimized graph to make predictions and will benchmark its performance. In the next notebook, you will be asked to make your first optimized TF-TRT graph using Float16 precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHvhlIOgHE1n"
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tXs_01BBHE1n"
   },
   "source": [
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- Convert a saved TensorFlow model into an optimized TF-TRT graph with Float32 precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7QCbaQKtHE1n"
   },
   "source": [
    "## Make Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A5xhUuH-HE1n"
   },
   "source": [
    "`convert_to_trt_graph_and_save` expects the directory of a saved model, which it will convert to an optimized TF-TRT graph with Float32 precision, and then save. Please read the comments for this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mOJlLkTLHE1o"
   },
   "outputs": [],
   "source": [
    "def convert_to_trt_graph_and_save(precision_mode='float32', input_saved_model_dir='resnet_v2_152_saved_model', calibration_data=batched_input):\n",
    "    \n",
    "    if precision_mode == 'float32':\n",
    "        precision_mode = trt.TrtPrecisionMode.FP32\n",
    "        converted_save_suffix = '_TFTRT_FP32'\n",
    "        \n",
    "    if precision_mode == 'float16':\n",
    "        precision_mode = trt.TrtPrecisionMode.FP16\n",
    "        converted_save_suffix = '_TFTRT_FP16'\n",
    "\n",
    "    if precision_mode == 'int8':\n",
    "        precision_mode = trt.TrtPrecisionMode.INT8\n",
    "        converted_save_suffix = '_TFTRT_INT8'\n",
    "        \n",
    "    output_saved_model_dir = input_saved_model_dir + converted_save_suffix\n",
    "    \n",
    "    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "        precision_mode=precision_mode, \n",
    "        max_workspace_size_bytes=8000000000\n",
    "    )\n",
    "\n",
    "    converter = trt.TrtGraphConverterV2(\n",
    "        input_saved_model_dir=input_saved_model_dir,\n",
    "        conversion_params=conversion_params\n",
    "    )\n",
    "\n",
    "    print('Converting {} to TF-TRT graph precision mode {}...'.format(input_saved_model_dir, precision_mode))\n",
    "    \n",
    "    if precision_mode == trt.TrtPrecisionMode.INT8:\n",
    "        \n",
    "        def calibration_input_fn():\n",
    "            yield (calibration_data, )\n",
    "\n",
    "        converter.convert(calibration_input_fn=calibration_input_fn)   \n",
    "    else:\n",
    "        converter.convert()\n",
    "\n",
    "    print('Saving converted model to {}...'.format(output_saved_model_dir))\n",
    "    converter.save(output_saved_model_dir=output_saved_model_dir)\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9MAty7aOHE1p",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert_to_trt_graph_and_save(precision_mode='float32', input_saved_model_dir='resnet_v2_152_saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Yj10vmHHE1r"
   },
   "outputs": [],
   "source": [
    "convert_to_trt_graph_and_save(precision_mode='float16', input_saved_model_dir='resnet_v2_152_saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JSA-9iAlHE1t"
   },
   "source": [
    "## Benchmark TF-TRT Float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DYboJElJHE1t"
   },
   "source": [
    "Here we load the optimized TF model. Note that this is a TF saved model, as opposed to a Keras saved model. If you wish, refer to `lab_helpers.py` for details on the helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQHQSqqLHE1t"
   },
   "outputs": [],
   "source": [
    "infer = load_tf_saved_model('resnet_v2_152_saved_model_TFTRT_FP32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7HWWmpqVHE1v"
   },
   "source": [
    "Now we perform inference with the optimized graph, and after a warmup, time and calculate throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "doxZ3JLqHE1v"
   },
   "outputs": [],
   "source": [
    "all_preds = predict_and_benchmark_throughput_from_saved(batched_input, infer, N_warmup_run=50, N_run=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2tCyMIceHE1x"
   },
   "source": [
    "**Compare *Throughput* to the naive TF 2 inference perfomed earlier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gYqt_ffBHE1x"
   },
   "source": [
    "Run this cell to view predictions, which you can use to compare to the naive TF 2 run. You should see very little difference in the accuracy of the predicitons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Es3IKIZHE1y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_run_preds = all_preds[0]\n",
    "display_prediction_info(last_run_preds, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9dnPhHnHE10"
   },
   "source": [
    "In the next notebook, you will be asked to make your first optimized TF-TRT graph using Float16 precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JQ7iIrVNHE10"
   },
   "source": [
    "## Benchmark TF-TRT Float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWTo54MBHE11"
   },
   "source": [
    "Load the optimized TF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0aHP7mRTHE11"
   },
   "outputs": [],
   "source": [
    "infer = load_tf_saved_model('resnet_v2_152_saved_model_TFTRT_FP16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcSPKwBsHE13"
   },
   "source": [
    "Perform inference with the optimized graph, and after a warmup, time and calculate throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XpWuU9ThHE14"
   },
   "outputs": [],
   "source": [
    "all_preds = predict_and_benchmark_throughput_from_saved(batched_input, infer, N_warmup_run=50, N_run=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJCnHNGJHE15"
   },
   "source": [
    "Run this cell to view predictions, which you can use for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZqlMIUFIHE17",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# INT8 Inference and Calibrationlast_run_preds = all_preds[0]\n",
    "display_prediction_info(last_run_preds, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQrWpEnVHE18"
   },
   "source": [
    "# INT8 Inference and Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C_LJh0ajHE18"
   },
   "source": [
    "In this notebook we will discuss how TF-TRT is able to optimize to use Int8 precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_a5kL8XHE19"
   },
   "source": [
    "## Benefits of Reduced Precision Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gF63DjoUHE1-"
   },
   "source": [
    "Typically, model training is performed using 32-bit floating point mathematics. Due to the backpropagation algorithm and weights updates, this high precision is necessary to allow for model convergence. Once trained, inference could be done in reduced precision (e.g. FP16) as the neural network architecture only requires a feed-forward network.\n",
    "\n",
    "Reducing numerical precision allows for a smaller model with faster inferencing time, lower memory requirements, and more throughput.\n",
    "\n",
    "Furthermore, recent NVIDIA GPUs are capable of executing 8-bit integer 4-element vector dot product instructions to accelerate deep neural network inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "id": "Fxw58qoSJhLb",
    "outputId": "3deda5d0-9a1d-4128-a9fe-4c4f14248175"
   },
   "outputs": [],
   "source": [
    "Image(\"images/dp4a-updated.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jpb-mZ8KHE1-"
   },
   "source": [
    "## Reduced Dynamic Range of INT8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMQvAhzAHE1_"
   },
   "source": [
    "While this new instruction provides faster computation, there is a significant challenge in representing weights and activations of deep neural networks in this reduced INT8 format. As *Table 1* shows, the dynamic range and granularity of representable values for INT8 is significantly limited compared to FP32 or FP16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "colab_type": "code",
    "id": "kCU2eWfKJh5O",
    "outputId": "0e007f41-ab9d-44e9-9328-78b94b6e3a2c"
   },
   "outputs": [],
   "source": [
    "Image(\"images/table1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mhTINjLSHE1_"
   },
   "source": [
    "## TF-TRT INT8 Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aY5ahc1qHE1_"
   },
   "source": [
    "You might be wondering how it is possible to take a model which operates in 32 bit floating point precision, where you can represent billions of different numbers, and reduce that to only 8 bit integers which can only represent 256 possible values.\n",
    "\n",
    "The main reason is that, typically in deep learning, the values of weights and activations lie in very small ranges. So if we design our precious 8 bits to only represent this specific small range, we can usually maintain good accuracy while reducing the rounding error.\n",
    "\n",
    "The main challenge is to find the correct dynamic range of the inputs. TF-TRT uses a calibration process that minimizes the information loss when approximating the FP32 network with a limited 8-bit integer representation. In the next notebook you will see how to perform this calibration process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "colab_type": "code",
    "id": "skEH-_6_Joic",
    "outputId": "bff73bed-6eff-4e97-b888-95c294aee40f"
   },
   "outputs": [],
   "source": [
    "Image(\"images/int_8_approach.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kUXgfGKJHE2A"
   },
   "source": [
    "## Calibration Dataset Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MWB6KKVWHE2A"
   },
   "source": [
    "When preparing the calibration dataset, you should capture the expected distribution of data in typical inference scenarios. You need to make sure that the calibration dataset covers all the expected scenarios, for example, clear weather, rainy day, night scenes, etc. When examining your own dataset, you should create a separate calibration dataset. The calibration dataset shouldn’t overlap with the training, validation or test datasets. \n",
    "\n",
    "For a much deeper technical dive on INT8 inference, you may watch the [8-Bit inference using TensorRT](http://on-demand.gputechconf.com/gtc/2017/video/s7310-szymon-migacz-8-bit-inference-with-tensorrt.mp4) presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nm58TQzYHE2A"
   },
   "source": [
    "# Convert to TF-TRT INT8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FdLykfl8HE2B"
   },
   "source": [
    "In this notebook you will convert a TensorFlow saved model into a TF-TRT optimized graph using INT8 precision. You will use the optimized graph to make predictions and will benchmark its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2aiSqn5wHE2B"
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BAmG2ZKjHE2B"
   },
   "source": [
    "By the time you complete this notebook you wil be able to:\n",
    "\n",
    "- Use TF-TRT to optimize a saved model with INT8 precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jk9n-pmcHE2C"
   },
   "source": [
    "## Converting to TF-TRT INT8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNZi6qpBHE2C"
   },
   "source": [
    "To perform INT8 optimization, we simply need to:\n",
    "\n",
    "- Set `precision_mode` to `trt.TrtPrecisionMode.INT8`\n",
    "- Pass a `calibration_input_fn` to `converter.convert`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GrKO4ce-HE2C"
   },
   "source": [
    "### Calibration Input Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Om6c1qtoHE2D"
   },
   "source": [
    "`calibration_input_fn` should be a generator function that yields input data as a list or tuple.\n",
    "\n",
    "You need to make sure that the calibration dataset covers all the expected scenarios, for example, clear weather, rainy day, night scenes, etc. When examining your own dataset, you should create a separate calibration dataset. The calibration dataset should not overlap with the training, validation, or test datasets.\n",
    "\n",
    "For our simple example here, we will not take these extra steps and will simply pass in our `batched_input` as calibration data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gw-WSzaNHE2D"
   },
   "source": [
    "## Convert to TF-TRT INT8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AXkj2wUiHE2E"
   },
   "outputs": [],
   "source": [
    "convert_to_trt_graph_and_save(precision_mode='int8', input_saved_model_dir='resnet_v2_152_saved_model', calibration_data=batched_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKWFWMLRHE2F"
   },
   "source": [
    "## Benchmark TF-TRT INT8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5nFoG_IQHE2F"
   },
   "source": [
    "Load the optimized TF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_zVLNTeHE2G"
   },
   "outputs": [],
   "source": [
    "infer = load_tf_saved_model('resnet_v2_152_saved_model_TFTRT_INT8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPAtD2mvHE2I"
   },
   "source": [
    "Perform inference with the optimized graph, and after a warmup, time and calculate throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y3xUNH1tHE2I"
   },
   "outputs": [],
   "source": [
    "all_preds = predict_and_benchmark_throughput_from_saved(batched_input, infer, N_warmup_run=50, N_run=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-H9XomQwHE2K"
   },
   "source": [
    "Run this cell to view predictions, which you can use for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKrBQgTNHE2K",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_run_preds = all_preds[0]\n",
    "display_prediction_info(last_run_preds, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Are9pg0HE2M"
   },
   "source": [
    "## Next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7f64vWN5HE2N"
   },
   "source": [
    "In the next notebook you will optimize additional models, and experiment with the impact of changing the `minimum_segment_size` conversion parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYNAdWJbHE2N"
   },
   "source": [
    "# Benchmark Different Minimum Segment Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CF-sw4REHE2N"
   },
   "source": [
    "In this notebook we will discuss the `minimum_segment_size` conversion parameter, and will ask you to experiment with the value, observing how it impacts throughput in optimized models. Additionally, you will perform conversion for 2 additional models, VGG19 and InceptionV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-raNxWJVHE2Q"
   },
   "outputs": [],
   "source": [
    "from lab_helpers import (\n",
    "    get_images, batch_input, load_tf_saved_model,\n",
    "    predict_and_benchmark_throughput_from_saved, display_prediction_info\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ug_wsHYHE2T"
   },
   "source": [
    "## Minimum Segment Size Conversion Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GRaEi7xjHE2T"
   },
   "source": [
    "The success of a TF-TRT optimization task is also dependent on the architecture of the model. The more supporting layers comprising the model, the greater number of TF-TRT layers generated and consequently, higher performance is achieved.\n",
    "\n",
    "The `minimum_segment_size` conversion parameter determines the minimum number of nodes required for a subgraph to be replaced by an optimized TF-TRT op. While its default value of 3 tends to offer the best performance for most models, adjusting this value can have varying impact on different models.\n",
    "\n",
    "For even more on the impacts of this parameter, see the [TF-TRT User Guide](https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/#min-nodes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aa5EPoB8HE2T"
   },
   "source": [
    "## VGG19 and InceptionV3 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YOwQ_3W-HE2U"
   },
   "source": [
    "In this notebook we will utilize 2 additional models: **VGG19** and **InceptionV3**. Execute the following cells to load them, and save them to file, so that they are in the format TF-TRT expects. Feel free to continue reading while the models save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sd2jKpYeHE2U"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fn6ZOO4vHE2V"
   },
   "outputs": [],
   "source": [
    "vgg19_model = VGG19(weights='imagenet')\n",
    "inception_v3_model = InceptionV3(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1hta7bVhHE2W"
   },
   "outputs": [],
   "source": [
    "vgg19_model.save('vgg19_saved_model')\n",
    "inception_v3_model.save('inception_v3_saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-rP_X3LHE2X"
   },
   "source": [
    "### Batch Input for Additional Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FcT7oxYUHE2Y"
   },
   "source": [
    "Before we performance inference (and benchmark), we need to batch our input.\n",
    "\n",
    "Our `batch_input` helper function performs model-specific image preprocessing. Therefore we create one set of batched images for each of the 2 additional models. If you're interested, check out `lab_helpers.py` for the source code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ciMurVmJHE2Y"
   },
   "outputs": [],
   "source": [
    "number_of_images = 16\n",
    "images = get_images(number_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxrupuWMHE2a"
   },
   "outputs": [],
   "source": [
    "vgg19_batched_input = batch_input(images, model=\"vgg19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atdSmEDbHE2d"
   },
   "outputs": [],
   "source": [
    "inception_v3_batched_input = batch_input(images, model=\"inception_v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4vB01mBHE2f"
   },
   "source": [
    "## Benchmark Different Minimum Segment Sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MOhXQ9U0HE2g"
   },
   "source": [
    "As you can see, the default value for `minimum_segment_size` is `3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cz8dkJ96HE2g"
   },
   "outputs": [],
   "source": [
    "trt.DEFAULT_TRT_CONVERSION_PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7QewAqPQHE2i"
   },
   "source": [
    "For this exercise you are asked to optimize the  **vgg_19** and **inception_v3** models, varying `minimum_segment_size` to maximize throughput (on FP16 mode only)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Upp45PjiHE2i"
   },
   "source": [
    "### Allow for Different Minimum Segment Size Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "inBJ0pqfHE2i"
   },
   "source": [
    "As you can see, `convert_to_trt_graph_and_save` now accepts a `minimum_segment_size` argument, which can be used to control the minimum segment size during conversion to a TF-TRT optimized model. Read the comments to see pertinent changes to our helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lhuz0SEpHE2i"
   },
   "outputs": [],
   "source": [
    "def convert_to_trt_graph_and_save(\n",
    "    precision_mode='float16',\n",
    "    input_saved_model_dir='vgg19_saved_model',\n",
    "    max_batch_size=16,\n",
    "    minimum_segment_size=3\n",
    "):\n",
    "    precision_mode = trt.TrtPrecisionMode.FP16\n",
    "    converted_save_suffix = '_TFTRT_FP16'\n",
    "    \n",
    "        \n",
    "    if minimum_segment_size != 3:\n",
    "        converted_save_suffix += '_MSS_{}'.format(str(minimum_segment_size))\n",
    "        \n",
    "    output_saved_model_dir = input_saved_model_dir + converted_save_suffix\n",
    "    \n",
    "    conversion_params = trt.DEFAULT_TRT_CONVERSION_PARAMS._replace(\n",
    "        precision_mode=precision_mode, \n",
    "        max_workspace_size_bytes=8000000000,\n",
    "        max_batch_size=max_batch_size,\n",
    "        minimum_segment_size=minimum_segment_size\n",
    "    )\n",
    "\n",
    "    converter = trt.TrtGraphConverterV2(\n",
    "        input_saved_model_dir=input_saved_model_dir,\n",
    "        conversion_params=conversion_params\n",
    "    )\n",
    "\n",
    "    print('Converting {} to TF-TRT graph precision mode {}...'.format(input_saved_model_dir, precision_mode))\n",
    "    \n",
    "    converter.convert()\n",
    "\n",
    "    print('Saving converted model to {}...'.format(output_saved_model_dir))\n",
    "    converter.save(output_saved_model_dir=output_saved_model_dir)\n",
    "    print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0M8zl3tOHE2k"
   },
   "source": [
    "### Benchmarking Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yxWRJVgHE2k"
   },
   "source": [
    "As you perform the following operations, use this table to track your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "raw",
    "id": "Uhed5FQZHE2k"
   },
   "source": [
    "Throughput\n",
    "\n",
    "      Model  |   Minimum Segment Size\n",
    "             |      1             5      \n",
    "--------------------------------------\n",
    "             |\n",
    "vgg_19       |     TODO          TODO\n",
    "             |\n",
    "             |\n",
    "inception_v3 |     TODO          TODO\n",
    "             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KEYPEqokHE2k"
   },
   "source": [
    "### Benchmark Different Minimum Segment Sizes for VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lrFmwh_ZHE2l"
   },
   "source": [
    "Run the following cells, adjusting `minimum_segment_size` so that you can observe the impact of its value when using VGG19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3v4kXwKPHE2l",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'vgg19'\n",
    "minimum_segment_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mE7CFlf3HE2m",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_saved_model_dir = '{}_saved_model'.format(model_name) \n",
    "\n",
    "convert_to_trt_graph_and_save(precision_mode='float16',\n",
    "                              minimum_segment_size=minimum_segment_size,\n",
    "                              input_saved_model_dir=input_saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-HtI7o8HE2n",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infer = load_tf_saved_model('{}_saved_model_TFTRT_FP16_MSS_{}'.format(model_name, str(minimum_segment_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yyo9ixnYHE2o",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We use batched input, and process predictions, specifically for VGG19\n",
    "all_preds = predict_and_benchmark_throughput_from_saved(vgg19_batched_input, infer, N_run=150, N_warmup_run=50, model='vgg19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JsHnLUFHE2p"
   },
   "source": [
    "Optionally, display prediction info for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OkBy-KgPHE2p",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_run_preds = all_preds[0]\n",
    "display_prediction_info(last_run_preds, images, model='vgg19')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "155-fl4YHE2q"
   },
   "source": [
    "### Benchmark Different Minimum Segment Sizes for InceptionV3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "De3p7kZ6HE2r"
   },
   "source": [
    "Run the following cells, adjusting `minimum_segment_size` so that you can observe the impact of its value when using InceptionV3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRWzrvVyHE2r",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'inception_v3'\n",
    "minimum_segment_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K4_-8EGwHE2s",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_saved_model_dir = '{}_saved_model'.format(model_name)\n",
    "\n",
    "convert_to_trt_graph_and_save(precision_mode='float16',\n",
    "                              minimum_segment_size=minimum_segment_size,\n",
    "                              input_saved_model_dir=input_saved_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YzeDWUh4HE2t",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infer = load_tf_saved_model('{}_saved_model_TFTRT_FP16_MSS_{}'.format(model_name, str(minimum_segment_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TDv7uTOOHE2y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We use batched input, and process predictions, specifically for InceptionV3\n",
    "all_preds = predict_and_benchmark_throughput_from_saved(inception_v3_batched_input, infer, N_run=150, N_warmup_run=50, model='inception_v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F6ul7QkEHE2z"
   },
   "source": [
    "Optionally, display prediction info for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X67zYjLkHE20",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_run_preds = all_preds[0]\n",
    "display_prediction_info(last_run_preds, images, model='inception_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "text",
    "id": "NXQCAteaHE21"
   },
   "outputs": [],
   "source": [
    "Image(\"images/table2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qUoIiDW-HE21"
   },
   "source": [
    "### Restarting Kernel at the End of Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TDlICvhpHE22"
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01-intro.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "tensorrt",
   "language": "python",
   "name": "tensorrt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
